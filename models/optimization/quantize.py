"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ONNX –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (quantization)
–≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏
"""

import onnx
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic, QuantType
import numpy as np
import time
import json
from pathlib import Path

Path("../../models/optimization").mkdir(parents=True, exist_ok=True)


def quantize_onnx_model(input_model_path, output_model_path):
    """
    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è ONNX –º–æ–¥–µ–ª–∏
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ—Å–∞ –∏–∑ FP32 –≤ INT8 –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è
    """
    
    print("üîß –ù–∞—á–∏–Ω–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏...")
    print(f"  –í—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å: {input_model_path}")
    print(f"  –í—ã—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å: {output_model_path}")
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    quantize_dynamic(
        input_model_path,
        output_model_path,
        weight_type=QuantType.QInt8  # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤ INT8
    )
    
    print("‚úÖ –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    return output_model_path


def compare_model_sizes(original_path, quantized_path):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    
    import os
    
    original_size = os.path.getsize(original_path) / (1024 * 1024)  # MB
    quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)  # MB
    
    reduction = ((original_size - quantized_size) / original_size) * 100
    
    print("\nüì¶ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤:")
    print(f"  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {original_size:.2f} MB")
    print(f"  –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {quantized_size:.2f} MB")
    print(f"  –£–º–µ–Ω—å—à–µ–Ω–∏–µ: {reduction:.1f}%")
    
    return {
        'original_size_mb': original_size,
        'quantized_size_mb': quantized_size,
        'reduction_percent': reduction
    }


def benchmark_inference(model_path, model_name, n_samples=100, n_runs=1000):
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    Args:
        model_path: –ø—É—Ç—å –∫ ONNX –º–æ–¥–µ–ª–∏
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
        n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
        n_runs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
    """
    
    print(f"\n‚ö° –ë–µ–Ω—á–º–∞—Ä–∫: {model_name}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ ONNX Runtime
    session = ort.InferenceSession(model_path)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Ö–æ–¥–µ
    input_name = session.get_inputs()[0].name
    input_shape = session.get_inputs()[0].shape
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ - batch size
    test_input = np.random.randn(n_samples, input_shape[1]).astype(np.float32)
    
    # –ü—Ä–æ–≥—Ä–µ–≤ (warm-up)
    for _ in range(10):
        _ = session.run(None, {input_name: test_input})
    
    # –ë–µ–Ω—á–º–∞—Ä–∫
    times = []
    for _ in range(n_runs):
        start = time.perf_counter()
        _ = session.run(None, {input_name: test_input})
        times.append(time.perf_counter() - start)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    times = np.array(times) * 1000  # –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
    
    results = {
        'mean_ms': float(np.mean(times)),
        'std_ms': float(np.std(times)),
        'min_ms': float(np.min(times)),
        'max_ms': float(np.max(times)),
        'median_ms': float(np.median(times)),
        'p95_ms': float(np.percentile(times, 95)),
        'p99_ms': float(np.percentile(times, 99)),
        'throughput_samples_per_sec': float(n_samples / (np.mean(times) / 1000))
    }
    
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {results['mean_ms']:.4f} ¬± {results['std_ms']:.4f} ms")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞: {results['median_ms']:.4f} ms")
    print(f"  P95: {results['p95_ms']:.4f} ms")
    print(f"  P99: {results['p99_ms']:.4f} ms")
    print(f"  –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {results['throughput_samples_per_sec']:.0f} samples/sec")
    
    return results


def validate_accuracy(original_model_path, quantized_model_path, n_samples=1000):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    """
    
    print("\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–π
    original_session = ort.InferenceSession(original_model_path)
    quantized_session = ort.InferenceSession(quantized_model_path)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_name = original_session.get_inputs()[0].name
    input_shape = original_session.get_inputs()[0].shape
    test_input = np.random.randn(n_samples, input_shape[1]).astype(np.float32)
    
    # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
    original_output = original_session.run(None, {input_name: test_input})[0]
    quantized_output = quantized_session.run(None, {input_name: test_input})[0]
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–ª–∏—á–∏—è
    mae = np.mean(np.abs(original_output - quantized_output))
    mse = np.mean((original_output - quantized_output) ** 2)
    max_diff = np.max(np.abs(original_output - quantized_output))
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    correlation = np.corrcoef(
        original_output.flatten(),
        quantized_output.flatten()
    )[0, 1]
    
    print(f"  MAE (Mean Absolute Error): {mae:.6f}")
    print(f"  MSE (Mean Squared Error): {mse:.6f}")
    print(f"  Max Difference: {max_diff:.6f}")
    print(f"  Correlation: {correlation:.6f}")
    
    if correlation > 0.99:
        print("‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è > 0.99")
    elif correlation > 0.95:
        print("‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–∞—è –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è > 0.95")
    else:
        print("‚ùå –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏!")
    
    return {
        'mae': float(mae),
        'mse': float(mse),
        'max_diff': float(max_diff),
        'correlation': float(correlation)
    }


def create_optimization_report(size_comparison, original_bench, quantized_bench, accuracy):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    speedup = original_bench['mean_ms'] / quantized_bench['mean_ms']
    
    report = {
        'size_reduction': size_comparison,
        'performance': {
            'original': original_bench,
            'quantized': quantized_bench,
            'speedup': float(speedup)
        },
        'accuracy': accuracy
    }
    
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –û–ë –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
    print("=" * 60)
    
    print("\nüì¶ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:")
    print(f"  –£–º–µ–Ω—å—à–µ–Ω–∏–µ: {size_comparison['reduction_percent']:.1f}%")
    print(f"  {size_comparison['original_size_mb']:.2f} MB ‚Üí {size_comparison['quantized_size_mb']:.2f} MB")
    
    print("\n‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
    print(f"  –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.2f}x")
    print(f"  {original_bench['mean_ms']:.4f} ms ‚Üí {quantized_bench['mean_ms']:.4f} ms")
    
    print("\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å:")
    print(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {accuracy['correlation']:.6f}")
    print(f"  MAE: {accuracy['mae']:.6f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    with open('../../models/optimization/optimization_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print("\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: models/optimization/optimization_report.json")
    
    return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    print("=" * 60)
    print("–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò: –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–Ø")
    print("=" * 60)
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    ORIGINAL_MODEL = '../../models/onnx/credit_scoring_model.onnx'
    QUANTIZED_MODEL = '../../models/optimization/credit_scoring_quantized.onnx'
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    import os
    if not os.path.exists(ORIGINAL_MODEL):
        print(f"‚ùå ONNX –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {ORIGINAL_MODEL}")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python models/onnx/convert_to_onnx.py")
        return
    
    # 1. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
    quantize_onnx_model(ORIGINAL_MODEL, QUANTIZED_MODEL)
    
    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    size_comparison = compare_model_sizes(ORIGINAL_MODEL, QUANTIZED_MODEL)
    
    # 3. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    original_bench = benchmark_inference(
        ORIGINAL_MODEL,
        "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (FP32)",
        n_samples=100,
        n_runs=1000
    )
    
    # 4. –ë–µ–Ω—á–º–∞—Ä–∫ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    quantized_bench = benchmark_inference(
        QUANTIZED_MODEL,
        "–ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (INT8)",
        n_samples=100,
        n_runs=1000
    )
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏
    accuracy = validate_accuracy(ORIGINAL_MODEL, QUANTIZED_MODEL, n_samples=1000)
    
    # 6. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    create_optimization_report(size_comparison, original_bench, quantized_bench, accuracy)
    
    print("\n" + "=" * 60)
    print("‚úÖ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 60)
    print(f"\nüìÅ –§–∞–π–ª—ã:")
    print(f"  - –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {QUANTIZED_MODEL}")
    print(f"  - –û—Ç—á–µ—Ç: models/optimization/optimization_report.json")
    
    print("\nüí° –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: —Å–æ–∑–¥–∞–π—Ç–µ Terraform –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
    print("   cd infrastructure/environments/production")
    print("   terraform init")


if __name__ == "__main__":
    main()
